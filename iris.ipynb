{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1631b616",
   "metadata": {},
   "source": [
    "## Machine Learning Workshop\n",
    "\n",
    "This workshop is a demo for machine learning in Python.  \n",
    "The goal for this session is not to train an amazing ML model, but to explain the typical steps in a machine learning project and to provide some background and further reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-celtic",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "- install conda: https://docs.conda.io/projects/conda/en/latest/user-guide/install/download.html\n",
    "- create a conda environment required packages (in console): `conda create -n ml_workshop scikit-learn matplotlib ipykernel`\n",
    "- Activate conda environment and start a Jupyter notebook with the right environment:\n",
    "\n",
    "        conda activate ml_workshop\n",
    "        python -m ipykernel install --user --name ml_workshop --display-name \"ML Workshop\"\n",
    "        jupyter notebook\n",
    "- Alternative: run in VSCode with Python and Jupyter notebook extensions\n",
    "- if you don't want to set up a local version, you can try this browser-based notebook: https://jupyter.org/try-jupyter/retro/notebooks/?path=notebooks/Intro.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-stephen",
   "metadata": {},
   "source": [
    "### Other popular Python libraries for ML\n",
    "- `numpy`, `pandas`: for data manipulation\n",
    "- `scikit-learn` for ML algorithms, model evaluation, resampling\n",
    "- `tensorflow`, `keras`, `pytorch` for Deep Learning\n",
    "- `matplotlib` for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-valentine",
   "metadata": {},
   "source": [
    "### The iris dataset\n",
    "\n",
    "The iris dataset contains measurements of 3 different types of flowers. It contains 150 data points (50 of each class).\n",
    "For more background, see https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "\n",
    "Sepals and Petals | Iris setosa | Iris versicolor | Iris virginica\n",
    "- | - | - | -\n",
    "![Sepals and Petals](iris_figs/220px-Petal-sepal.jpg) | ![Iris setosa flower](iris_figs/Kosaciec_szczecinkowaty_Iris_setosa.jpg) | ![Iris versicolor flower](iris_figs/220px-Iris_versicolor_3.jpg) |![Iris virginica flower](iris_figs/Iris_virginica.jpg) \n",
    "\n",
    "4 features: length and width of sepals and petals (different parts of the flower)  \n",
    "3 classes: setosa, versicolor and virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d1447",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-fifth",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "The goal for a classification model is to learn the relationship between the features and the class label (so we can apply it to new data points later). At the same time, we want to understand how well our model performs.    \n",
    "If we evaluate the model on the data it was trained on, a model that simply memorizes the training data will get a perfect score - this is useless! We need to find a better way to evaluate our model's performance.  \n",
    "\n",
    "One method for this evaluation is resampling. The general idea here is to train the model on a subset of the available data, and evaluate it on a different subset. There are many methods for this, see https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets and also https://en.wikipedia.org/wiki/Cross-validation_(statistics). \n",
    "\n",
    "In this tutorial, we will use a simple train/test-split.\n",
    "\n",
    "![Dataset split in training and test set](iris_figs/train_test.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.33)\n",
    "print(f\"Number of rows / features in training data: {X_train.shape}\")\n",
    "print(f\"Number of training labels: {y_train.shape}\")\n",
    "\n",
    "print(f\"Number of rows / features in test data: {X_test.shape}\")\n",
    "print(f\"Number of test labels: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-shanghai",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "There are tons of different options for ML algorithms to choose from - see for example https://scikit-learn.org/stable/supervised_learning.html. \\\n",
    "Selecting the right algorithm can be crucial for the performance of the system, but how do we decide which algorithm works best? \\\n",
    "Often times, this comes down to a few factors:\n",
    "- previous experience on a task / similar dataset\n",
    "- current state of the art (based on scientific literature)\n",
    "- experimentation (try a bunch of things and see what works best)  \n",
    "\n",
    "For this tutorial, we will try three classification algorithms: [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree), [Support Vector Machine](https://en.wikipedia.org/wiki/Support-vector_machine) and [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = SVC()\n",
    "clf3 = KNeighborsClassifier(n_neighbors=3)\n",
    "classifiers, clf_names = [clf1, clf2, clf3], [\"Decision Tree\", \"Support Vector Machine\", \"k-nearest neighbors\"]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    classifier = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c45be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation - for more information about the confusion matrix, see https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "# scikit-learn offers many metrics out of the box - see https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics for an overview\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for classifier, name in zip(classifiers, clf_names):\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy on test data for {name}: {accuracy_score(y_true=y_test, y_pred=y_pred)*100}%\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-groove",
   "metadata": {},
   "source": [
    "## Retraining on full dataset after model selection\n",
    "\n",
    "Once we are happy with the estimated performance of our selected model, we can retrain it on the entire dataset.  \n",
    "In many cases having more training data can improve model performance, so it would be a waste not to use the test data.  \n",
    "You may skip this step in some instances (for example if model training is very expensive computationally) and use the model you trained previously on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = clf1.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some analysis for the Decision Tree\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plot = plot_tree(clf1, feature_names=iris.feature_names, class_names=iris.target_names)\n",
    "plt.savefig(\"iris_figs/decision_tree.png\", format=\"png\", dpi=300, facecolor='white', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-leave",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Many ML algorithms have their own parameters, which can greatly improve (or decrease) the model performance. As part of the model selection process, we often train models with different hyperparameters to see what works best. \\\n",
    "There are many techniques for selecting hyperparameter configurations (see https://en.wikipedia.org/wiki/Hyperparameter_optimization), these are very common:\n",
    "- Grid Search (exhaustive sweep over pre-defined hyperparameter values - very inefficient)\n",
    "- Random Search (sampling random hyperparameter configurations)\n",
    "- Bayesian Optimization (sampling \"promising\" hyperparameter configurations based on previous evaluations)  \n",
    "\n",
    "Personal recommendation: I'm a big fan of Bayesian Optimization, for example with the `optuna` package. \\\n",
    "The following example showcases how a bad choice of hyperparameter can destroy model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-nearest neighbors predicts the majority class of the k closest training data points (based on a distance measure)\n",
    "bad_classifier = KNeighborsClassifier(n_neighbors=100)\n",
    "bad_classifier.fit(X_train, y_train)\n",
    "y_pred = bad_classifier.predict(X_test)\n",
    "print(f\"Accuracy on test data for kNN: {accuracy_score(y_true=y_test, y_pred=y_pred)*100:.2f}%\")\n",
    "# Because we consider 100 neighbors and the training dataset only contains 100 rows, we always predict the most common class\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-mixer",
   "metadata": {},
   "source": [
    "## Related topics not covered in this session\n",
    "\n",
    "- Data engineering - getting high quality training data is the most important step in any machine learning project\n",
    "- Over-/Underfitting, Bias-Variance tradeoff\n",
    "- Deployment - or if you like buzzwords, [MLOps](https://en.wikipedia.org/wiki/MLOps)\n",
    "- Online experimentation - using statistical techniques like [A/B testing](https://en.wikipedia.org/wiki/A/B_testing)\n",
    "- Monitoring - model performance may change / degrade over time (e. g. due to changing usage patterns following a global pandemic)\n",
    "- Privacy / Responsible AI / Compliance - especially important in large organizations"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ced9b228531b4281b95a8c18c96d2f3f173f7474dc88fd41d6490d0d133ae51b"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
